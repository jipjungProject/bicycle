{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/database/201801.csv' does not exist: b'data/database/201801.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5307a563c413>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/pickle/data_10_20178.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path, compression)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/pickle/data_10_20178.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5307a563c413>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/database/201801.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/database/201802.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/database/201803.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/database/201801.csv' does not exist: b'data/database/201801.csv'"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model as lm\n",
    "from scipy.stats import uniform, randint\n",
    "import xgboost as xgb\n",
    "\n",
    "rental = np.array([207, 502, 113, 2102, 1210, \n",
    "                   152, 1308, 2219, 1906, 907])\n",
    "\n",
    "features = ['year',\n",
    "           'month', \n",
    "           'hour', \n",
    "           'temp(°C)',\n",
    "           'precipitation(mm)', \n",
    "           'windspeed(m/s)', \n",
    "           'humidity(%)', \n",
    "           'snow(cm)',\n",
    "           'day_0', \n",
    "           'day_1', \n",
    "           'day_2', \n",
    "           'day_3',\n",
    "           'day_4', \n",
    "           'day_5', \n",
    "           'day_6']\n",
    "\n",
    "try:\n",
    "    t = pd.read_pickle('data/pickle/data_10_20178.pkl')\n",
    "    \n",
    "except:\n",
    "    \n",
    "    df1 = pd.DataFrame(pd.read_csv(\"data/database/201801.csv\"))\n",
    "    df2 = pd.DataFrame(pd.read_csv(\"data/database/201802.csv\"))\n",
    "    df3 = pd.DataFrame(pd.read_csv(\"data/database/201803.csv\"))\n",
    "    df4 = pd.DataFrame(pd.read_csv(\"data/database/201804.csv\"))\n",
    "    df5 = pd.DataFrame(pd.read_csv(\"data/database/201805.csv\"))\n",
    "    df6_1 = pd.DataFrame(pd.read_csv(\"data/database/201806_01.csv\"))\n",
    "    df6_2 = pd.DataFrame(pd.read_csv(\"data/database/201806_02.csv\"))\n",
    "    df6 = pd.concat([df6_1, df6_2])\n",
    "    df7_1 = pd.DataFrame(pd.read_csv(\"data/database/201807_01.csv\"))\n",
    "    df7_2 = pd.DataFrame(pd.read_csv(\"data/database/201807_02.csv\"))\n",
    "    df7 = pd.concat([df7_1, df7_2])\n",
    "    df8 = pd.DataFrame(pd.read_csv(\"data/database/201808.csv\"))\n",
    "    df9_1 = pd.DataFrame(pd.read_csv(\"data/database/201809_1.csv\"))\n",
    "    df9_2 = pd.DataFrame(pd.read_csv(\"data/database/201809_2.csv\"))\n",
    "    df9 = pd.concat([df9_1, df9_2])\n",
    "    df10_1 = pd.DataFrame(pd.read_csv(\"data/database/201810_01.csv\"))\n",
    "    df10_2 = pd.DataFrame(pd.read_csv(\"data/database/201810_02.csv\"))\n",
    "    df10 = pd.concat([df10_1, df10_2])\n",
    "    df11 = pd.DataFrame(pd.read_csv(\"data/database/201811.csv\"))\n",
    "    df12 = pd.DataFrame(pd.read_csv(\"data/database/201812.csv\"))\n",
    "\n",
    "\n",
    "    df2017_1 = pd.DataFrame(pd.read_csv(\"data/database/201701.csv\"))\n",
    "    df2017_2 = pd.DataFrame(pd.read_csv(\"data/database/201702.csv\"))\n",
    "    df2017_3 = pd.DataFrame(pd.read_csv(\"data/database/201703.csv\"))\n",
    "    df2017_4 = pd.DataFrame(pd.read_csv(\"data/database/201704.csv\"))\n",
    "    df2017_5 = pd.DataFrame(pd.read_csv(\"data/database/201705.csv\"))\n",
    "    df2017_6 = pd.DataFrame(pd.read_csv(\"data/database/201706.csv\"))\n",
    "    df2017_7 = pd.DataFrame(pd.read_csv(\"data/database/201707.csv\"))\n",
    "    df2017_8 = pd.DataFrame(pd.read_csv(\"data/database/201708.csv\"))\n",
    "    df2017_9 = pd.DataFrame(pd.read_csv(\"data/database/201709.csv\"))\n",
    "    df2017_10 = pd.DataFrame(pd.read_csv(\"data/database/201710.csv\"))\n",
    "    df2017_11 = pd.DataFrame(pd.read_csv(\"data/database/201711.csv\"))\n",
    "    df2017_12 = pd.DataFrame(pd.read_csv(\"data/database/201712.csv\"))\n",
    "\n",
    "    df = [[df2017_1, df2017_2, df2017_3, df2017_4, df2017_5, df2017_6, df2017_7, df2017_8, df2017_9, df2017_10, df2017_11, df2017_12],[df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12]]\n",
    "    \n",
    "    def load_2019_df(rental_no, w , df2019_1, df2019_2, df2019_3):\n",
    "        wt = load_weather_month(w, 1)\n",
    "        datat, baset = calculate_date(df2019_1)\n",
    "        temp = concat_columns(datat, wt, baset, rental_no)\n",
    "\n",
    "        wt1 = load_weather_month(w, 2)\n",
    "        datat1, baset1 = calculate_date(df2019_2)\n",
    "        temp1 = concat_columns(datat1, wt1, baset1, rental_no)\n",
    "\n",
    "        wt3 = load_weather_month(w, 3)\n",
    "        datat2, baset2 = calculate_date(df2019_3)\n",
    "        temp2 = concat_columns(datat2, wt3, baset2, rental_no)\n",
    "    \n",
    "        result = pd.concat([temp, temp1, temp2])\n",
    "        return result\n",
    "        \n",
    "    def load_2019():\n",
    "        df2019_1 = pd.DataFrame(pd.read_csv(\"data/database/201901.csv\"))\n",
    "        df2019_2 = pd.DataFrame(pd.read_csv(\"data/database/201902.csv\"))\n",
    "        df2019_3 = pd.DataFrame(pd.read_csv(\"data/database/201903.csv\"))\n",
    "        weather = pd.read_csv(\"data/database/2019weather.csv\")\n",
    "        dfw1 = pd.DataFrame(weather)\n",
    "        dfw1['일시'] = pd.to_datetime(dfw1['일시'], errors='coerce')\n",
    "        dfw1['month'] = dfw1['일시'].dt.month\n",
    "        dfw1['day'] = dfw1['일시'].dt.day\n",
    "        dfw1['hour'] = dfw1['일시'].dt.hour\n",
    "        \n",
    "        t1 = load_2019_df(207, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t2 = load_2019_df(502, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t3 = load_2019_df(113, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t4 = load_2019_df(2102, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t5 = load_2019_df(1210, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t6 = load_2019_df(152, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t7 = load_2019_df(1308, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t8 = load_2019_df(2219, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t9 = load_2019_df(1906, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t10 = load_2019_df(907, dfw1, df2019_1, df2019_2, df2019_3)\n",
    "        t= pd.concat([t1,t2,t3,t4,t5,t6,t7,t8,t9,t10]).reset_index(drop=True)\n",
    "        \n",
    "        return t  \n",
    "    \n",
    "    def load_weather():\n",
    "        weather = pd.read_csv(\"data/database/2017weather.csv\")\n",
    "        dfw1 = pd.DataFrame(weather)\n",
    "        dfw1['일시'] = pd.to_datetime(dfw1['일시'], errors='coerce')\n",
    "        dfw1['month'] = dfw1['일시'].dt.month\n",
    "        dfw1['day'] = dfw1['일시'].dt.day\n",
    "        dfw1['hour'] = dfw1['일시'].dt.hour\n",
    "        weather1 = pd.read_csv(\"data/database/2018weather.csv\")\n",
    "        dfw2 = pd.DataFrame(weather1)\n",
    "        dfw2['일시'] = pd.to_datetime(dfw2['일시'], errors='coerce')\n",
    "        dfw2['month'] = dfw2['일시'].dt.month\n",
    "        dfw2['day'] = dfw2['일시'].dt.day\n",
    "        dfw2['hour'] = dfw2['일시'].dt.hour\n",
    "        dfw = [dfw1, dfw2]\n",
    "        return dfw\n",
    "\n",
    "    dfw = load_weather()\n",
    "\n",
    "    def load_weather_month(weather, mon):\n",
    "        weather1 = weather[weather['month'] == mon]\n",
    "        weather1 = weather1.drop(['일시','month'], 1).reset_index(drop=True)\n",
    "        return weather1\n",
    "\n",
    "    def calculate_date(df):\n",
    "        dft = df\n",
    "        dft['대여시간'] = pd.to_datetime(dft['대여시간'], errors='coerce')\n",
    "        dft['year'] = dft['대여시간'].dt.year - 2010\n",
    "        dft['요일'] = dft['대여시간'].dt.weekday\n",
    "        dft['month'] = dft['대여시간'].dt.month\n",
    "        dft['day'] = dft['대여시간'].dt.day\n",
    "        dft['hour'] = dft['대여시간'].dt.hour\n",
    "        base = pd.DataFrame(dft.drop_duplicates(['day','hour'])).reset_index(drop=True)\n",
    "        dftemp = df[df['대여장소'].isin(rental)]\n",
    "        return dftemp, base\n",
    "\n",
    "    def concat_columns(dftemp, dfw, base, rental_no):\n",
    "        temp1 = base\n",
    "        dftemp113 = dftemp[dftemp['대여장소'] == rental_no]\n",
    "        #print(pd.crosstab(dftemp113['일'], dftemp113['시'], margins=False))\n",
    "        fre = pd.crosstab(dftemp113['day'], dftemp113['hour'], margins=False).values.flatten()\n",
    "        frelist = fre.tolist()\n",
    "\n",
    "        for i in range(int(dfw.shape[0]/24)):\n",
    "            if((dftemp113[dftemp113['day'] == (i+1)]).shape[0]==0):\n",
    "                for x in range(24):\n",
    "                    frelist.insert((i*24)+x ,0)\n",
    "\n",
    "        for i in range(24):\n",
    "            if((dftemp113[dftemp113['hour'] == i]).shape[0]==0):\n",
    "                for x in range(int(dfw.shape[0]/24)):\n",
    "                    frelist.insert((i+(x*24)),0)\n",
    "\n",
    "        fre = np.array(frelist)                        \n",
    "        fre = pd.DataFrame(fre, columns=['건수'])\n",
    "        temp1['대여장소'] = rental_no\n",
    "        temp1['빈도수'] = fre['건수']\n",
    "        temp1 = temp1.drop(['대여시간', '반납시간', '반납장소'], 1)\n",
    "        temp1 = pd.merge(temp1, dfw, on=['day', 'hour'])\n",
    "        temp1 = temp1.drop(['day'], 1)\n",
    "        return temp1\n",
    "\n",
    "    def data_no(rental_no):\n",
    "        dfwlist = []\n",
    "        data = [0 for _ in range(12)]\n",
    "        base = [0 for _ in range(12)]\n",
    "        temp = [0 for _ in range(12)]\n",
    "        r = pd.DataFrame()\n",
    "        for x in range(len(dfw)):\n",
    "            for i in range(12):\n",
    "                dfwlist.append(load_weather_month(dfw[x], i+1))\n",
    "\n",
    "            for i in range(12):\n",
    "                data[i], base[i] = calculate_date(df[x][i])\n",
    "\n",
    "            for i in range(12):\n",
    "                temp[i] = concat_columns(data[i], dfwlist[12*x+i], base[i], rental_no)\n",
    "\n",
    "            result = pd.concat([temp[0],temp[1],temp[2],temp[3],temp[4],temp[5],temp[6],temp[7],temp[8],temp[9],temp[10],temp[11]]).reset_index(drop=True)\n",
    "            r = pd.concat([r, result])\n",
    "\n",
    "        return r.reset_index(drop=True)\n",
    "    \n",
    "    t2019 = load_2019()\n",
    "    \n",
    "\n",
    "    t1 = data_no(207)\n",
    "    t2 = data_no(502)\n",
    "    t3 = data_no(113)\n",
    "    t4 = data_no(2102)\n",
    "    t5 = data_no(1210)\n",
    "    t6 = data_no(152)\n",
    "    t7 = data_no(1308)\n",
    "    t8 = data_no(2219)\n",
    "    t9 = data_no(1906)\n",
    "    t10 = data_no(907)\n",
    "    t2018 = pd.concat([t1,t2,t3,t4,t5,t6,t7,t8,t9,t10]).reset_index(drop=True)\n",
    "    \n",
    "    t2018 = t2018.join(pd.get_dummies(t2018['요일'], prefix=\"day\"))\n",
    "    t2019 = t2019.join(pd.get_dummies(t2019['요일'], prefix=\"day\"))\n",
    "    \n",
    "    t = pd.concat([t2018,t2019]).reset_index(drop=True)    \n",
    "    t.to_pickle('data/pickle/data_10_20178.pkl')\n",
    "\n",
    "t = t[t['대여장소'] == 207].reset_index(drop=True)\n",
    "t = t.rename(columns = {'기온(°C)': 'temp(°C)'})\n",
    "t = t.rename(columns = {'강수량(mm)': 'precipitation(mm)'})\n",
    "t = t.rename(columns = {'풍속(m/s)': 'windspeed(m/s)'})\n",
    "t = t.rename(columns = {'습도(%)': 'humidity(%)'})\n",
    "t = t.rename(columns = {'적설(cm)': 'snow(cm)'})\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 3.6582087959889455\n",
      "ridge 3.6575863218048843\n",
      "lasso 3.6596208815031157\n",
      "elastic 3.6581347243744315\n",
      "LassoLars 3.6596205712464767\n",
      "LogisticRegression 4.167174796747967\n",
      "SGDRegressor 26.97344722961564\n",
      "Perceptron 5.381605691056911\n",
      "[18:02:23] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "xgboost 2.42836894016198\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(t[features], t['빈도수'], test_size=0.1, random_state=15)\n",
    "\n",
    "models = [\n",
    "    ('xgboost', xgb.XGBRegressor())\n",
    "]\n",
    "\n",
    "n=3\n",
    "\n",
    "params = { 'lr' : { 'fit_intercept': [True, False], 'normalize': [True, False], }, 'ridge': { 'alpha': [0.01, 0.1, 1.0, 10, 100], 'fit_intercept': [True, False], 'normalize': [True, False], }, 'lasso': { 'alpha': [0.1, 1.0, 10], 'fit_intercept': [True, False], 'normalize': [True, False], }, 'elastic': { 'alpha': [0.1, 1.0, 10], 'normalize': [True, False], 'fit_intercept': [True, False], }, 'LassoLars': { 'alpha': [0.1, 1.0, 10], 'normalize': [True, False], 'fit_intercept': [True, False], }, 'LogisticRegression': { 'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1.0, 10, 100], 'fit_intercept': [True, False], }, 'SGDRegressor': { 'penalty': ['l1', 'l2'], 'alpha': [0.001, 0.01, 0.1, 1.0, 10, 100], 'fit_intercept': [True, False], }, 'Perceptron' :{ 'penalty': ['None', 'l1', 'l2'], 'alpha': [0.001, 0.01, 0.1, 1.0, 10, 100], 'fit_intercept': [True, False] }, 'xgboost': { \"gamma\": uniform(0, 0.5).rvs(n), \"max_depth\": range(2, 7),\"n_estimators\": randint(100, 150).rvs(n),} }\n",
    "\n",
    "best_model, best_mae = None, float('inf')\n",
    "\n",
    "for model_name, model in models:\n",
    "    param_grid = params[model_name]\n",
    "    grid = GridSearchCV(model, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "    grid = grid.fit(x_train, y_train)\n",
    "\n",
    "    model = grid.best_estimator_\n",
    "    predictions = model.predict(x_test)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    print(model_name, mae)\n",
    "\n",
    "    if mae < best_mae:\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0.24081666484688058,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=6, min_child_weight=1, missing=None, n_estimators=127,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fce4a9ad7087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeature_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfeature_importance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"feature importance\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "feature_importance = pd.Series(index=features, data=best_model.feature_importances_).sort_values(ascending=False) \n",
    "feature_importance.plot(kind='bar', figsize=(15, 10), title=\"feature importance\", rot=0) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
